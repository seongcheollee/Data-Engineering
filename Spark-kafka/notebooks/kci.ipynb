{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "156c5e28-9883-40a3-bc73-5ba317c8c52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pymongo import MongoClient\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, explode, collect_list, size, expr, struct, col, when, array_contains , flatten, desc, size\n",
    "from pyspark.sql.types import StringType, ArrayType, StructType, StructField,FloatType\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "285f8e64-e60f-4a7c-89a5-bd207b71231c",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"kci\").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59e2347d-16e0-4dab-a504-4c2c733fd007",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available databases: ['Egg_', 'admin', 'category', 'config', 'kci_AuGraph', 'kci_api', 'kci_author_info', 'kci_ccGraph', 'kci_trained_api', 'local', 'mydb', 'reference_map']\n"
     ]
    }
   ],
   "source": [
    "client = MongoClient(\"mongodb://root:1234@mongodb:27017/admin\")\n",
    "\n",
    "db_names = client.list_database_names()\n",
    "print(\"Available databases:\", db_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f126a39a-63ac-4eb2-bc3c-79cea7a86837",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['_id', 'articleID', 'titleKor', 'titleEng', 'abstractKor', 'abstractEng', 'journalID', 'issn', 'journalName', 'kciRegistration', 'pubYear', 'pubMon', 'pubDate', 'volume', 'issue', 'vol_issue', 'category', 'category1', 'category2', 'articleRegularity', 'laguage', 'author1ID', 'author1Name', 'author1Inst', 'author2IDs', 'author2Names', 'author2Insts', 'keywords', 'fpage', 'lpage', 'page', 'doi', 'uci', 'citations', 'url', 'verified', 'refereceID', 'refereceTypeCode', 'refereceTypeName', 'refereceTitle', 'refereceAuthor', 'refereceConfJournal', 'referecePublisher', 'referecePubDate', 'refereceVolume', 'refereceIssue', 'refereceVolIss', 'referecePage'])\n"
     ]
    }
   ],
   "source": [
    "db_name = \"kci_api\"\n",
    "collection_name = \"kci_202310\"\n",
    "\n",
    "mongo_data = list(client[db_name][collection_name].find({}))\n",
    "\n",
    "print(mongo_data[0].keys())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e7bde9-083d-4435-90ae-326e15bece36",
   "metadata": {},
   "source": [
    "# Kci "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cfb4f303-20ce-461f-81c8-8f137d46584e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+---------+-------------------------------+---------------+-------+------+-------+------+-----+---------+------------------+---------+---------+-----------------+-------+------------+---------------------+-----------------------+--------------------------------------------------------+--------------------------------+------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----+-----+-------+---+---+---------+-----------------------------------------------------------------------------------------------------------------+--------+----------+----------------+----------------+-------------+--------------+-------------------+-----------------+---------------+--------------+-------------+--------------+------------+\n",
      "|articleID   |titleKor                                                                               |titleEng                                                                                                                                                  |abstractKor                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |abstractEng                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |journalID   |issn     |journalName                    |kciRegistration|pubYear|pubMon|pubDate|volume|issue|vol_issue|category          |category1|category2|articleRegularity|laguage|author1ID   |author1Name          |author1Inst            |author2IDs                                              |author2Names                    |author2Insts                                                      |keywords                                                                                                                                                                                                                                                    |fpage|lpage|page   |doi|uci|citations|url                                                                                                              |verified|refereceID|refereceTypeCode|refereceTypeName|refereceTitle|refereceAuthor|refereceConfJournal|referecePublisher|referecePubDate|refereceVolume|refereceIssue|refereceVolIss|referecePage|\n",
      "+------------+---------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+---------+-------------------------------+---------------+-------+------+-------+------+-----+---------+------------------+---------+---------+-----------------+-------+------------+---------------------+-----------------------+--------------------------------------------------------+--------------------------------+------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----+-----+-------+---+---+---------+-----------------------------------------------------------------------------------------------------------------+--------+----------+----------------+----------------+-------------+--------------+-------------------+-----------------+---------------+--------------+-------------+--------------+------------+\n",
      "|ART002996496|TwinAMFNet: 3차원 시맨틱 세그멘테이션을 위한 Twin 어텐션 기반 멀티모달 퓨전 네트워크   |TwinAMFNet : Twin Attention-based Multi-modal Fusion Network for 3D Semantic Segmentation                                                                 |최근 자율주행에서 오인식으로 인한 충돌 사고가 증가함에 따라 멀티 모달 센서를 활용한 센서 퓨전 기반의 3차원 시맨틱 세그멘테이션에 관한 관심이 늘어나고 있다. 이에 따라 본 연구에서는 카메라와 LiDAR의 센서 퓨전을 통해 새로운 3차원 시맨틱 세그멘테이션 신경망인 TwinAMFNet을 소개한다. 제안하는 신경망은 RGB 영상과 2차원의 좌표 평면에 사영한 점 군 사영 영상을 처리하는 Twin 신경망을 포함하며 인코더 및 디코더에서의 특징 단계 퓨전을 위한 어텐션 기반 퓨전 모듈을 통해 더욱 확장된 객체 및 경계 구분에 대한 표현력 개선을 보여준다. 결과적으로 제안한 신경망은 mIoU를 기준으로 3차원 시맨틱 세그멘테이션에 약 68%의 성능을 기록하였으며 기존 연구들에 비해 약 4.5% 이상 향상된 성능을 보였다.                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |Recently, with the increase in the number of accidents due to misrecognition in autonomous driving, interest in 3D semantic segmentation based on sensor fusion using multi-modal sensors has increased. Accordingly, this study introduces TwinAMFNet, a novel 3D semantic segmentation neural network through sensor fusion of RGB cameras and LiDAR. The proposed neural network includes a twin neural network that processes RGB images and point cloud projection images projected on a 2D coordinate plane and through an attention-based fusion module for feature step fusion in the encoder and decoder. The proposed method shows improvement of further extended object and boundary classification. As a result, the proposed neural network recorded approximately 68% performance in 3D semantic segmentation based on mIoU, and showed approximately 4.5% improved performance compared to the ones reported in the existing studies.                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |SER000004124|2383-630X|정보과학회논문지               |우수등재       |2023   |09    |2023-09|50    |9    |50(9)    |[공학, 컴퓨터학]  |공학     |컴퓨터학 |Y                |한국어 |CRT002911099|윤재근               |인피닉                 |[CRT002911098, CRT002911100]                            |[전지연, 송광호]                |[인피닉, 인피닉]                                                  |[자율 주행, 시맨틱 세그멘테이션, 멀티모달, 센서 퓨전, 어텐션, autonomous driving, semantic segmentation, multimodal, sensor fusion, attention]                                                                                                              |784  |794  |784-794|N/A|N/A|0        |https://www.kci.go.kr/kciportal/ci/sereArticleSearch/ciSereArtiView.kci?sereArticleSearchBean.artiId=ART002996496|N       |[]        |[]              |[]              |[]           |[]            |[]                 |[]               |[]             |[]            |[]           |[]            |[]          |\n",
      "|ART002996491|착용형 보행 보조 로봇을 위한 센서 데이터 기반 동작 인식 알고리즘                       |Activity Recognition Algorithms based on Sensor Data for Wearable Walking-Assistive Robots                                                                |본 논문에서는 센서 데이터를 활용하여 착용형 보행 보조 로봇이 해당 동작을 정확하게 인식할 수 있도록 하는 딥러닝 기반 동작 인식에 대한 종합적인 연구를 제시한다. 연구에 사용된 데이터셋은 보행 보조 로봇 착용자의 재활훈련 중 수집되었다. 심층 신경망의 훈련 과정을 최적화하기 위해 데이터를 정제 및 가공하여 불필요한 구간을 제거하였다. Convolutional Neural Network(CNN), Long Short-Term Memory(LSTM), CNN-Gated Recurrent Unit(GRU)를 포함한 다양한 심층 신경망 아키텍처를 비교하여 착용형 보행 보조 로봇의 동작 인식에 가장 효과적인 모델을 선정하고자 하였다. 실제 데이터에 대한 실험 결과, 고려한 모든 신경망 모델이 비교적 우수한 성능을 보였으며, 해당 연구의 데이터셋에는 LSTM이 약간의 이점이 있다는 것을 확인하였다. 또한, 해당 연구는 착용형 보행 보조 로봇의 기능 향상을 위한 인사이트와 발전에 기여하며, 이 분야에서 더 많은 개선 가능성을 강조한다.                                                                                                                                                                                                                                                                                                        |The present study aimed to comprehensively perform deep learning-based activity recognition for enabling wearable walking-assistive robots to accurately identify corresponding activities using sensor data. Dataset used for this study was obtained from rehabilitation training of walking-assistive robot wearers. To optimize the training process of deep neural networks, we conducted careful processing and refinement of the data, eliminating unnecessary segments. We systematically compared various deep neural network architectures, including Convolutional Neural Network(CNN), Long Short-Term Memory(LSTM) and CNN-Gated Recurrent Unit(GRU), to identify the most effective model for activity recognition in wearable walking-assistive robots. Our experiments conducted using real-world data demonstrated that all deep neural networks used in this study performed comparably well, with LSTMs showing a slight advantage for our specific problem.\\r\\nThis research contributes valuable insights and advancements towards enhancing capabilities of wearable walking-assistive robots, highlighting the potential for further improvements in this domain.                                                                                                                                                                                                                                   |SER000004124|2383-630X|정보과학회논문지               |우수등재       |2023   |09    |2023-09|50    |9    |50(9)    |[공학, 컴퓨터학]  |공학     |컴퓨터학 |Y                |한국어 |CRT002977789|손준혁               |인텔리코드             |[CRT002977790, CRT002977791]                            |[손진호, 최승진]                |[엔젤로보틱스, 인텔리코드]                                        |[착용형 로봇, 행동 인식, 센서 데이터, 딥러닝, wearable robot, activity recognition, sensor data, deep learning]                                                                                                                                             |751  |757  |751-757|N/A|N/A|0        |https://www.kci.go.kr/kciportal/ci/sereArticleSearch/ciSereArtiView.kci?sereArticleSearchBean.artiId=ART002996491|N       |[]        |[]              |[]              |[]           |[]            |[]                 |[]               |[]             |[]            |[]           |[]            |[]          |\n",
      "|ART002996488|극대공통부분서열을 찾는 개선된 알고리즘                                                |An Improved Algorithm of Finding a Maximal Common Subsequence                                                                                             |두 문자열의 극대공통부분서열(MCS)은 어떤 문자를 삽입하여도 더 긴 공통부분서열을 만들 수 없는 공통부분서열이다. 최장공통부분서열(LCS)과 달리 MCS의 길이는 다양하고, 가장 긴 MCS가 LCS이다. LCS는 일반적으로 두 서열의 유사도를 비교할 때 사용되지만, 계산하는데 매우 긴 시간이 걸린다. MCS는 LCS보다 빠르게 계산할 수 있으므로 더 긴 MCS를 찾는 문제는 중요하다. 길이의 합이 n인 두 문자열 X와 Y의 MCS 중 하나를 O(n) 공간을 이용하여 시간에 계산하는 알고리즘이 제시되었고 이를 개선한 알고리즘들도 제시되었다. 본 논문에서는 기존의 알고리즘들보다 더 많은 문자를 확인하여, 상수 k가 주어졌을 때 O(kn) 공간을 이용하여  시간에 MCS를 계산하는 알고리즘을 제시한다. 실제 데이터와 무작위로 생성한 데이터를 이용하여 실험을 진행한 결과, 본 논문에서 제시한 알고리즘으로 계산된 MCS의 길이가 기존의 알고리즘으로 계산된 MCS보다, 최대 6.31배 길다.                                                                                                                                                                                                                                                                                                                         |A maximal common subsequence (MCS) of two strings is a common subsequence that cannot be extended by inserting any character. Unlike the longest common subsequence (LCS), the length of MCS can vary as the longest MCS is an LCS. Although LCS is commonly used to compare similarities of two sequences, computing can take a significant amount of time. Hence, finding a longer MCS is important, as it can be computed faster than the LCS. An algorithm was proposed to compute one of the MCSs of two strings  and  of total length  using  space and logloglog  time. Improved algorithms were also proposed. In this paper, we present an algorithm that can check for more characters to compute an MCS. The algorithm proposed in this paper runs in  space and logloglog  time for a given constant . Experimental results using both real and randomly generated data showed that the length of the MCS computed by the algorithm proposed in this paper could be up to 6.31 times longer than those computed by previous algorithms                                                                                                                                                                                                                                                                                                                                         |SER000004124|2383-630X|정보과학회논문지               |우수등재       |2023   |09    |2023-09|50    |9    |50(9)    |[공학, 컴퓨터학]  |공학     |컴퓨터학 |Y                |한국어 |CRT002977784|신현준               |인하대학교 컴퓨터공학과|[CRT000863934, CRT000295671]                            |[나중채, 심정섭]                |[세종대학교, 인하대학교]                                          |[maximal common subsequences, longest common subsequences, string comparison, string algorithms, 극대공통부분서열, 최장공통부분서열, 문자열 비교, 문자열 알고리즘]                                                                                          |737  |745  |737-745|N/A|N/A|0        |https://www.kci.go.kr/kciportal/ci/sereArticleSearch/ciSereArtiView.kci?sereArticleSearchBean.artiId=ART002996488|N       |[]        |[]              |[]              |[]           |[]            |[]                 |[]               |[]             |[]            |[]           |[]            |[]          |\n",
      "|ART002996492|C3DSG: 실내 환경 포인트 클라우드를 이용한 3차원 장면 그래프 생성 모델                  |C3DSG: A 3D Scene Graph Generation Model Using Point Clouds of Indoor Environment                                                                         |포인트 클라우드로부터 3차원 장면 그래프를 생성하는 효과적인 심층 신경망 모델을 설계하기 위해서는 3가지 중요한 도전과제들을 해결해야 한다. 첫 번째 도전과제는 3차원 포인트 클라우드에 대한 효과적인 기하학적 특징 추출 방식을 결정하는 일이다. 두 번째 도전과제는 물체들 간의 3차원 공간 관계를 효과적으로 파악하기 위해서는 어떤 비-기하학적 특징들을 추가적으로 이용할 것인가를 결정하는 일이다. 세 번째 도전과제는 효과적인 공간적 맥락 추론 방식을 결정하는 일이다. 본 논문에서는 이와 같은 도전과제들에 대응하기 위해, 실내 환경 포인트 클라우드로부터 3차원 장면 그래프 예측을 위한 새로운 심층 신경망 모델을 제안한다. 제안 모델은 Point Transformer를 이용해 추출하는 3차원 포인트 클라우드의 기하학적 특징뿐만 아니라, 물체들 간의 3차원 공간 관계 예측에 도움을 줄 수 있는 언어적 특징과 상대적 비교 특징 등 다양한 비-기하학적 특징들도 함께 활용한다. 또한 제안 모델은 물체들 간의 공간적 맥락정보를 효과적으로 이끌어내기 위해, 물체 노드들과 이들을 연결하는 간선들 모두에 주의집중을 적용할 수 있는 새로운 NE-GAT 그래프 신경망을 이용한다. 본 논문에서는 3DSSG 벤치마크 데이터 집합을 이용한 다양한 실험들을 통해, 제안 모델의 효과와 우수성을 입증한다.   |To design an effective deep neural network model to generate 3D scene graphs from point clouds, the following three challenging issues need to be resolved: 1) to decide how to extract effective geometric features from point clouds, 2) to determine what non-geometric features are used complementarily for recognizing 3D spatial relationships between two objects, and 3) to decide which spatial reasoning mechanism is used. To address these challenging issues, we proposed a novel deep neural network model for generating 3D scene graphs from point clouds of indoor environments. The proposed model uses both geometric features of 3D point cloud extracted using Point Transformer and various non-geometric features such as linguistic features and relative comparison features that can help predict the 3D spatial relationship between objects. In addition, the proposed model uses a new NE-GAT graph neural network module that can apply attention to both object nodes and edges connecting them to effectively derive spatial context between objects. Conducting a variety of experiments using 3DSSG benchmark dataset, effectiveness and superiority of the proposed mode were proven.                                                                                                                                                                                                  |SER000004124|2383-630X|정보과학회논문지               |우수등재       |2023   |09    |2023-09|50    |9    |50(9)    |[공학, 컴퓨터학]  |공학     |컴퓨터학 |Y                |한국어 |CRT002974521|백호준               |경기대학교             |[CRT000301696]                                          |[김인철]                        |[경기대학교]                                                      |[scene understanding, point cloud, 3D scene graph generation, graph neural network, 장면 이해, 포인트 클라우드, 3차원 장면 그래프 생성, 그래프 신경망]                                                                                                      |758  |770  |758-770|N/A|N/A|0        |https://www.kci.go.kr/kciportal/ci/sereArticleSearch/ciSereArtiView.kci?sereArticleSearchBean.artiId=ART002996492|N       |[]        |[]              |[]              |[]           |[]            |[]                 |[]               |[]             |[]            |[]           |[]            |[]          |\n",
      "|ART002996500|PE image 탐지 방법을 우회하는 파워쉘 기반 공격 기법 설계 및 대응 방안                  |Design and Countermeasures of PowerShell-based Attack Techniques to Bypass Portable Executable Image Detection Methods                                    |지속적으로 발전하는 파일리스 악성코드 공격은 주로 윈도우즈 기본 탑재 도구인 파워쉘(PowerShell)의 스크립트 기능을 활용하는 경우가 많다. 파워쉘은 윈도우즈 시스템에서 인프라 관리 기능을 폭넓게 제공하는 유용한 도구인 만큼 컴퓨터 리소스에 쉽게 접근할 수 있기 때문이다. 이 파워쉘 기반 파일리스 악성코드는 공격 과정에서 PE image를 활용하는 특성을 가지므로, 이를 탐지하는 방법론 중 하나로 시스템에서 PE image의 흐름을 추적하고 탐지하는 방법이 있다(DLL Injection 모니터링 등). 본 논문에서는 파워쉘에 기반한 파일리스 악성코드 공격 수행 시, PE image를 전혀 사용하지 않고도 공격이 가능함을 보이고, 그러한 방식의 공격이 기존과 대비하여 어떤 차이점이 있는지를 실제 Proof-of-Concept 악성코드를 제작하여 비교하였다. 또한 이러한 형태의 악성코드에 효과적으로 대응할 수 있는 기법을 논의하였다.                                                                                                                                                                                                                                                                                                                                                                    |A fileless malware attack is an advanced ever-developing attack that usually exploits script functions in PowerShell, a default Windows tool. The reason is that PowerShell allows people to access computer resources easily because its tools provide infrastructure management broadly in Windows systems. This PowerShell-based fileless malware does involve using Portable Executable(PE) images in the process of attacking. One way to identify it is by tracking and detecting the flow of PE images (i.e., DLL Injection monitoring). In this paper, we demonstrate that it is possible to execute an attack without any PE images despite using fileless malware based on PowerShell and made Proof-of-Concept malware attack codes to demonstrate this method and compare the impact of such attack. Additionally, we discussed techniques to cope with this kind of advanced malicious code.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |SER000004124|2383-630X|정보과학회논문지               |우수등재       |2023   |09    |2023-09|50    |9    |50(9)    |[공학, 컴퓨터학]  |공학     |컴퓨터학 |Y                |한국어 |CRT002977795|장지원               |성신여자대학교         |[CRT002977796]                                          |[장대희]                        |[경희대학교 컴퓨터공학부]                                         |[파워쉘 기반 악성코드, 실행 파일리스 공격, 쉘코드 인젝션, 쉘코드 난독화, PowerShell-based malicious code, executable fileless attack, shellcode injection, shellcode obfuscation]                                                                           |813  |820  |813-820|N/A|N/A|0        |https://www.kci.go.kr/kciportal/ci/sereArticleSearch/ciSereArtiView.kci?sereArticleSearchBean.artiId=ART002996500|N       |[]        |[]              |[]              |[]           |[]            |[]                 |[]               |[]             |[]            |[]           |[]            |[]          |\n",
      "|ART002996533|웨어러블 기기 기반 종단 시스템 개발을 통한 HRV 측정 및 불면 증상과 관련성 확인         |End-to-End System Based on Wearable Devices for Measuring HRV and its Potential as an Insomnia Indicator                                                  |지속적인 의료 데이터 수집의 중요성에 기반해, 모바일 및 웨어러블 기기 애플리케이션은 일상 활동, 수면 및 다양한 생리적 측정을 추적하는 데 널리 사용되고 있다. 이 연구에서는 50명의 참가자를 대상으로 웨어러블 장치를 활용해 한 달간 움직임과 활동, 심박수를 포함한 생체 신호를 수집했다. 수집된 데이터로부터 스트레스 반응을 비롯한 인간의 생리적 상태를 나타내는 중요한 지표인 심박변이도(Heart Rate Variability, HRV)를 분석을 위해 추출했다. 또한, 개발된 시스템의 실용성을 입증하기 위해 수집된 HRV 특징을 가장 일반적인 수면 관련 정신 장애 중 하나인 불면증에 대한 지표로 사용하는 방법의 잠재성을 확인했다.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |Recognizing the importance of continuous medical data collection, mobile and wearable device applications are widely used for tracking everyday activities, sleep, and various physiological measurements. In this work, we collected month-long data from the wearable devices of 50 participants, capturing their heart rates and activity-related information. We additionally extracted heart rate variability (HRV) features from the raw data. HRV is a crucial indicator of human physiological conditions, highlighting the value of gathered data. To demonstrate the practicality of our system, we studied how HRV features could be potentially used as indicators of insomnia, one of the most widespread sleep-related mental disorders.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |A00398      |2383-6318|정보과학회 컴퓨팅의 실제 논문지|등재           |2023   |09    |2023-09|29    |9    |29(9)    |[공학, 컴퓨터학]  |공학     |컴퓨터학 |Y                |한국어 |CRT002977817|아이토큰 바이구타노바|KAIST                  |[CRT002959162, CRT002977818, CRT002977819]              |[박성규, 이상원, 차미영]        |[강원대학교, 경북대학교, IBS]                                     |[wearable device, wearable application, heart rate variability (HRV), insomnia, photoplethysmogram (PPG) signal, 웨어러블 기기, 웨어러블 애플리케이션, 심박변이도(HRV), 불면증, PPG 신호]                                                                   |403  |409  |403-409|N/A|N/A|0        |https://www.kci.go.kr/kciportal/ci/sereArticleSearch/ciSereArtiView.kci?sereArticleSearchBean.artiId=ART002996533|N       |[]        |[]              |[]              |[]           |[]            |[]                 |[]               |[]             |[]            |[]           |[]            |[]          |\n",
      "|ART002996534|문장 간 유사도와 문장 속성을 활용한 한영·영한 번역 병렬 말뭉치 품질 예측 모델          |A Quality Prediction Model for the Parallel Corpora of Korean-English and English-Korean Translation Utilizing Sentence Similarity and Sentence Attributes|본 연구를 통해 한영·영한 번역 병렬 말뭉치의 품질을 평가하는 모델을 제안하고, 이를 이용해 시중의 한영·영한 번역 병렬 말뭉치에 대해 품질 평가를 실시한다. 연구의 의의는 다음과 같다. 첫 번째로, DA(Direct Assessment) 점수로 평가하는 한영·영한 QE(Quality Estimation) 연구를 실시하며 관련 데이터를 최초 구축했다. 두 번째로 번역 병렬 말뭉치 품질 평가 모델 TwiQE를 구축했다. 세 번째로 복수의 전문가가 수립한 품질 기준을 제시하며 번역 병렬 말뭉치 품질 개선의 기틀을 마련했다. 마지막으로, 구축된 모델을 활용하여 AI HUB1)에 공개된 한영 번역 병렬 말뭉치 2종2), 영한 번역 병렬 말뭉치 2종에 대한 품질 평가를 실시했다. TwiQE를 통해 도출한 품질 평가 점수를 바탕으로 개선이 필요한 말뭉치를 두 수준으로 나누어 파악했고, 각 수준에 미치지 못하는 말뭉치의 수량과 비율을 파악하여 품질 개선의 기틀을 마련했다.                                                                                                                                                                                                                                                                                                                                                         |This study proposes a model for assessing the quality of Korean-English and English-Korean parallel translation corpora and applies it to assess the quality of existing Korean-English and English-Korean parallel translation corpora. The significance of the study is, firstly, it conducted Quality Estimation (QE) of Korean-English and English-Korean translation corpora using Direct Assessment (DA) scores, establishing related data for the first time. Secondly, it established a model for assessing the quality of parallel translation corpus, called TwiQE. Thirdly, it provided the basis for improving the quality of the parallel translation corpus by presenting quality standards established by multiple experts. Finally, using this model, it conducted a quality assessment on the two types of Korean-English and English-Korean parallel translation corpora available on AI HUB. Based on the quality assessment scores by TwiQE, the corpora that needed improvement was classified into two levels, and the quantity and proportion of corpora that did not meet the standards for each level to provide a basis for improving the quality of the corpora identified.                                                                                                                                                                                                                     |A00398      |2383-6318|정보과학회 컴퓨팅의 실제 논문지|등재           |2023   |09    |2023-09|29    |9    |29(9)    |[공학, 컴퓨터학]  |공학     |컴퓨터학 |Y                |한국어 |CRT002977820|고원희               |트위그팜               |[CRT002977821, CRT002977822]                            |[최진혁, 최규동]                |[트위그팜, 트위그팜]                                              |[quality estimation, cosine similarity, direct assessment, round-trip translation, 번역문 품질 평가, 코사인 유사도, DA 점수, 역번역]                                                                                                                        |410  |417  |410-417|N/A|N/A|0        |https://www.kci.go.kr/kciportal/ci/sereArticleSearch/ciSereArtiView.kci?sereArticleSearchBean.artiId=ART002996534|N       |[]        |[]              |[]              |[]           |[]            |[]                 |[]               |[]             |[]            |[]           |[]            |[]          |\n",
      "|ART002996538|엣지 컴퓨팅 기반의 도로 교통 서비스를 위한 시뮬레이션 플랫폼의 개발                    |Development of a Simulation Platform for Transportation Services Based on Edge Computing                                                                  |다양한 교통 문제를 해결하기 위해 개발되고 있는 지능형 차량용 서비스들을 검증하지 않고 실제 환경에 바로 적용하기에는 여러 위험이 존재하기 때문에 사전 분석 및 검증이 필수적이다. 하지만 실제 환경에서 차량용 서비스를 검증하는 데에는 비용의 한계가 존재한다. 따라서 차량용 서비스를 개발할 때 시뮬레이션을 통한 검증은 비용의 측면에서 도움이 된다. 본 연구에서는 차량용 서비스 검증을 위한 시뮬레이션 플랫폼을 제안한다. 지능형 차량 서비스는 일반적으로 대량의 데이터를 처리하는 과정을 포함하고 있으며 이러한 대량의 데이터를 차량 내부의 계산 자원으로 처리하기엔 한계가 존재하여 클라우드 같은 외부 자원은 필수적이다. 하지만 클라우드 컴퓨팅은 실시간성을 요구하는 차량용 서비스에는 지연 시간 측면에서 한계가 존재하여 엣지 컴퓨팅 기반 연구가 활발히 진행되고 있다. 본 연구에서 제안하는 시뮬레이션 플랫폼은 엣지 컴퓨팅 플랫폼과 차량 및 교통 시뮬레이터를 연동하여 엣지 컴퓨팅 기반 차량용 서비스 검증 환경을 제공한다. 이를 위해 오픈 소스 엣지 컴퓨팅 플랫폼인 EdgeX Foundry와 도시 교통 시뮬레이터인 SUMO를 사용하여 EdgeX-SUMO 플랫폼을 개발하고, EdgeX-SUMO 플랫폼을 통해 대표적인 도로 교통 문제인 꼬리 물기 현상을 방지하는 서비스를 구현하고 검증하였다.|Pre-analysis and verification of intelligent transportation services developed to solve various traffic problems are essential since there are several risks to deploy services directly to the real environment without verifying them. In addition, verifying transportation services in a real environment has a cost problem. Therefore, when developing transportation services, verification through simulation has benefits in terms of cost. In this article, we proposes a simulation platform for transportation service verification. Intelligent transportation services generally process a large amount of data. Since vehicles have limited computing power for processing large data, external computing resources such as cloud computing are essential. However, cloud computing has limitations in terms of latency for realtime transportation services. Edge computing-based research is actively being conducted. The simulation platform proposed in this article provides an edge computing-based service verification environment by combining the edge computing platform and transportation simulators. To this end, an EdgeX-SUMO interface was developed using EdgeX Foundry, an open source edge computing platform, and SUMO, an urban traffic simulator. By using our proposed platform, we verify that this intelligent service could prevent tail biting, a typical road traffic problem.|A00398      |2383-6318|정보과학회 컴퓨팅의 실제 논문지|등재           |2023   |09    |2023-09|29    |9    |29(9)    |[공학, 컴퓨터학]  |공학     |컴퓨터학 |Y                |한국어 |CRT001137051|김경진               |한양대학교             |[CRT001584107, CRT000296627]                            |[한혁, 강수용]                  |[동덕여자대학교, 한양대학교]                                      |[EdgeX Foundry, SUMO, 시뮬레이션, 엣지 컴퓨팅, 차량용 서비스, EdgeX Foundry, SUMO, simulation, edge computing, vehicular service]                                                                                                                           |438  |443  |438-443|N/A|N/A|0        |https://www.kci.go.kr/kciportal/ci/sereArticleSearch/ciSereArtiView.kci?sereArticleSearchBean.artiId=ART002996538|N       |[]        |[]              |[]              |[]           |[]            |[]                 |[]               |[]             |[]            |[]           |[]            |[]          |\n",
      "|ART002996498|센서별 시간지연 교차 상관관계를 적용한 GCN 기반의 시계열 데이터 이상 탐지              |A GCN-based Time-Series Data Anomaly Detection Method using Sensor-specific Time Lagged Cross Correlation                                                 |시계열 데이터를 통한 장비 이상 탐지는 더 큰 피해를 방지하고 생산성 향상에 기여할 수 있어 매우 중요한 과제이다. 이와 관련하여 시계열 데이터 이상 탐지에 대한 연구가 활발히 진행되고 있지만, 다음과 같은 제약사항들이 있다. 첫째, 센서 간 상관관계를 분석하지 않기 때문에 불필요한 허위 알람이 발생한다. 둘째, 센서 간 상관관계를 분석하기 위해 완전 그래프로 모델링하고 GAT(Graph Attention Networks)를 적용하였으나, 불필요한 연산의 증가로 많은 분석시간이 소요된다. 본 논문에서는 위의 제약 사항을 해결하기 위해 SC-GCNAD(Sensor-specific Correlation GCN Anomaly Detection)를 제안한다. SC-GCNAD는 시계열 데이터의 특징을 반영한 TLCC(Time Lagged Cross Correlation)를 적용하여 정확한 센서별 상관관계를 분석하고, 상관관계 표현력이 뛰어난 GCN(Graph Convolutional Networks)을 활용한다. 그 결과 기존 모델 대비 F1-Score는 최대 6.37% 향상하고, 분석시간은 최대 95.31% 단축한다.                                                                                                                                                                                                                                                                                      |Anomaly detection of equipment through time series data is a very important because it can prevent further damage and contribute to productivity improvement. Although research studies on time series data anomaly detection are being actively conducted, but they have the following restrictions. First, unnecessary false alarms occur because correlations with other sensors are not considered. Second, although complete graph modeling and GAT have been applied to analyze the correlation of each sensor, this method requires a lot of time due to the increase in unnecessary operations. In this paper, we propose SC-GCNAD(Sensor-specific Correlation GCN Anomaly Detection) to address these problems. SC-GCNAD can analyze the exact correlation of each sensor by applying TLCC that reflects characteristics of time series data. It utilize GCN with excellent model expressiveness. As a result, SC-GCNAD can improve F1-Score by up to 6.37% and reduce analysis time by up to 95.31% compared to the baseline model.                                                                                                                                                                                                                                                                                                                                                                              |SER000004124|2383-630X|정보과학회논문지               |우수등재       |2023   |09    |2023-09|50    |9    |50(9)    |[공학, 컴퓨터학]  |공학     |컴퓨터학 |Y                |한국어 |CRT002977794|이강우               |대한민국 해군          |[CRT002146149, CRT000319554]                            |[김윤영, 정성원]                |[서강대학교, 서강대학교]                                          |[sensor, time series data, anomaly detection, correlation analysis, TLCC, GCN, 센서, 시계열 데이터, 이상 탐지, 상관관계 분석, TLCC, GCN]                                                                                                                    |805  |812  |805-812|N/A|N/A|0        |https://www.kci.go.kr/kciportal/ci/sereArticleSearch/ciSereArtiView.kci?sereArticleSearchBean.artiId=ART002996498|N       |[]        |[]              |[]              |[]           |[]            |[]                 |[]               |[]             |[]            |[]           |[]            |[]          |\n",
      "|ART002996535|XGBoost를 이용한 중앙냉방시스템 건물의 냉방 에너지 소비량 예측 모델                    |Predictive Model of Cooling Energy Consumption of Buildings with Central Cooling Systems using XGBoost                                                    |국제에너지기구(International Energy Agency, IEA)에 따르면 2021년 건물 운영은 전 세계 최종 에너지 소비의 30%라는 큰 비중을 차지하고 있고, 총 에너지 부문 배출량의 27%를 이르고 있다. 배출량의 8%는 건물에서 직접적으로 배출되고, 19%는 건물에서 사용되는 전기 및 열 생산 등으로 간접적으로 배출되고 있다. 총 에너지 소비량을 줄이기 위하여서는 건물 에너지 소비의 최적화는 필수적이며, 이를 위하여서는 건물 에너지 소비의 예측 모델의 개발이 필요하다. 본 논문에서는 XGBoost 기법을 사용하여 중앙냉방시스템 건물의 냉방 에너지 사용량을 예측하는 기계학습 모델을 구축한다. 본 논문에서 제안하는 모델의 성능 검증을 위하여, 실제 냉방 에너지 사용량 및 환경 데이터를 사용하여 미래의 냉방 에너지 사용량을 예측하고, 실제 냉방 에너지 사용량을 비교하여 예측의 정확성을 검증한다.                                                                                                                                                                                                                                                                                                                                                                                            |According to the International Energy Agency (IEA), building operations accounted for a large portion of global final energy consumption in 2021 and 27% of total energy sector emissions.\\r\\nIn terms of emissions, 8% are emitted directly from buildings, and 19% are indirect emissions from electricity and heat production used in buildings. The optimization of building energy is essential to reducing total energy consumption. In order to develop optimal control and various related technologies, a predictive model is prioritized. In this paper, a machine learning model was built to predict the cooling energy consumption of buildings’ central cooling systems using XGBoost, a machine learning method. To verify the performance of the model, actual cooling energy consumption and environmental data were used to predict future cooling energy consumption and compare actual cooling energy consumption.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |A00398      |2383-6318|정보과학회 컴퓨팅의 실제 논문지|등재           |2023   |09    |2023-09|29    |9    |29(9)    |[공학, 컴퓨터학]  |공학     |컴퓨터학 |Y                |한국어 |CRT002977823|백영현               |인천대학교             |[CRT000876679]                                          |[강현철]                        |[인천대학교]                                                      |[에너지 소비량, XGBoost, 건물 에너지, 기계학습, 예측, energy consumption, XGBoost, building energy, machine learning, prediction]                                                                                                                           |418  |426  |418-426|N/A|N/A|0        |https://www.kci.go.kr/kciportal/ci/sereArticleSearch/ciSereArtiView.kci?sereArticleSearchBean.artiId=ART002996535|N       |[]        |[]              |[]              |[]           |[]            |[]                 |[]               |[]             |[]            |[]           |[]            |[]          |\n",
      "|ART002996486|Tensor-Train Decomposition을 적용한 임베딩 레이어를 위한 연산 최적화 기법              |Optimizing Computation of Tensor-Train Decomposed Embedding Layer                                                                                         |개인 맞춤형 추천 시스템은 일상에 녹아 들어있다. 하지만 딥 러닝 기반 추천 시스템 모델에서 임베딩 레이어는 과거 유저가 상호 작용하는 아이템 수가 늘어남에 따라, 임베딩 테이블의 메모리 사용량이 늘어 산업용 AI 데이터 센터의 리소스 대부분을 차지하고 있다. 이 문제를 극복하기 위한 해결책 중 하나는 심층 신경망에서 유망한 압축 기법인 Tensor-Train (TT) 분해이다. 본 연구에서는 TT-분해 기법이 적용된 임베딩 레이어의 연산인 Tensor-Train Gather and Reduce (TT-GnR)에서 발생하는 불필요한 연산에 관해 분석하고 이를 해결하기 위해 아이템 벡터들을 하나로 묶는 연산 단위인 그룹을 정의하고 그룹 단위로 연산하여 불필요한 연산을 줄이는 Group Reduced TT-Gather and Reduce (GRT-GnR) 연산을 제안한다. 실험을 통해 기존 TT-GnR 연산에 비해 latency가 41% 감소한다.                                                                                                                                                                                                                                                                                                                                                                                                          |Personalized recommendation system is ubiquitous in daily life. However, the huge amount of memory requirement to store the embedding tables used by deep learning-based recommendation system models is taking up most of the resources of industrial AI data centers. To overcome this problem, one of the solutions is to use Tensor-Train (TT) decomposition, is promising compression technique in deep neural network. In this study, we analyze unnecessary computations in Tensor-Train Gather and Reduce (TT-GnR) which is the operation of embedding layer applied with TT decomposition. To solve this problem, we define a computational unit called group to bind the item vectors into a group and propose Group Reduced TT-Gather and Reduce operation to reduce unnecessary operations by calculating with groups. Since the GRT-GnR operation is calculated in groups, computational cost varies depending on how item vectors are grouped. Experimental results showed that the GRT-GnR operation had a 41% decrease in latency compared to conventional TT-GnR operation.                                                                                                                                                                                                                                                                                                                               |SER000004124|2383-630X|정보과학회논문지               |우수등재       |2023   |09    |2023-09|50    |9    |50(9)    |[공학, 컴퓨터학]  |공학     |컴퓨터학 |Y                |한국어 |CRT002977783|유승민               |성균관대학교           |[CRT002965754, CRT000860259]                            |[이하윤, 신동군]                |[성균관대학교 전자전기컴퓨터공학과, 성균관대학교]                 |[personalized recommendation system, embedding table, Tensor-Train decomposition, optimizing operation, 개인화 추천 시스템, 임베딩 테이블, Tensor-Train 분해 기법, 연산 최적화]                                                                             |729  |736  |729-736|N/A|N/A|0        |https://www.kci.go.kr/kciportal/ci/sereArticleSearch/ciSereArtiView.kci?sereArticleSearchBean.artiId=ART002996486|N       |[]        |[]              |[]              |[]           |[]            |[]                 |[]               |[]             |[]            |[]           |[]            |[]          |\n",
      "|ART002996489|심층 신경망과 랜덤포레스트를 이용한 유전암 관련 단일 염기 변이의 병원성 예측           |Applying Deep Neural Networks and Random Forests to Predict the Pathogenicity of Single Nucleotide Variants in Hereditary Cancer-associated Genes         |최근 유전자 검사의 보급으로 인해 개인의 유전자 변이를 탐색하고, 병원성 정보를 통해 유전적 질병을 진단 및 예방하는 것이 가능해졌다. 하지만, 밝혀진 변이 중 병원성 정보가 있는 것의 수는 상당히 적다. 이러한 문제점을 해결하기 위해 기계학습을 통해 변이의 병원성을 예측하는 방법이 제안되었다. 본 논문에서는 심층 신경망과 기존 연구들에서 널리 사용되었던 랜덤포레스트 및 로지스틱회귀를 변이 원성 예측에 적용 및 비교한다. 실험 데이터는 유전암과 관련된 유전자 내에 존재하는 1,068 개의 단일 염기 변이들로 구성된다. 초매개변수 설정을 위해 생성된 100 개의 무작위 데이터 집합에 대한 실험 결과 랜덤 포레스트가 area under the precision recall curve에서 가장 우수한 성능을 보였다. 15 개의 홀드아웃 유전자 집합에 대한 실험에서는 심층 신경망이 평균적으로 가장 우수한 결과를 보였으나 두 번째로 우수한 랜덤포레스트와의 성능 차이는 유의미하지 않았다. 또한 로지스틱회귀는 두 모델에 비해 통계적으로 유의미하게 낮은 성능을 보였다. 결론적으로 심층 신경망과 랜덤포레스트가 로지스틱 회귀에 비해 유전암 관련 단일 염기 변이의 병원성 예측에 일반적으로 더 적합함을 알 수 있었다.                                                                                     |The recent proliferation of genetic testing has made it possible to explore an individual's genetic variants and use pathogenicity information to diagnose and prevent genetic diseases. However, the number of identified variants with pathogenicity information is quite small. A method for predicting the pathogenicity of variants by machine learning was proposed to address this problem. In this study, we apply and compare deep neural networks with random forests and logistic regression, which have been widely used in previous studies, to predict variant pathogenicity. The experimental data consisted of 1,068 single-nucleotide variants in genes associated with hereditary cancers. Experiments on 100 random data-sets generated for hyperparameter selection showed that random forests performed best in terms of area under the precision-recall curve. On 15 holdout gene data-sets, deep neural networks performed best on average, but the difference in performance from the second-best random forest was not significant. Logistic regression was also statistically significantly worse than that of either model.\\r\\nIn conclusion, we found that deep neural networks and random forests were generally better than logistic regression at predicting the pathogenicity of single-nucleotide variants associated with hereditary cancer.                                             |SER000004124|2383-630X|정보과학회논문지               |우수등재       |2023   |09    |2023-09|50    |9    |50(9)    |[공학, 컴퓨터학]  |공학     |컴퓨터학 |Y                |한국어 |CRT002463809|이다빈               |숭실대학교             |[CRT002977785, CRT002977786, CRT002977787, CRT000891627]|[김선화, 강문종, 홍창범, 황규백]|[엔젠바이오, 주식회사 엔젠바이오, 주식회사 엔젠바이오, 숭실대학교]|[prediction of variant pathogenicity, bioinformatics, deep neural networks, deep learning, machine learning, 변이 병원성 예측, 생물정보학, 심층 신경망, 딥러닝, 기계학습]                                                                                   |746  |750  |746-750|N/A|N/A|0        |https://www.kci.go.kr/kciportal/ci/sereArticleSearch/ciSereArtiView.kci?sereArticleSearchBean.artiId=ART002996489|N       |[]        |[]              |[]              |[]           |[]            |[]                 |[]               |[]             |[]            |[]           |[]            |[]          |\n",
      "|ART002996493|정치적 양극화 완화를 위한 인공지능 웹 인터페이스 구축                                  |An AI Web-based Interface for Mitigating Political Polarization                                                                                           |정치적 양극화는 사회적 분열을 촉진할 뿐만 아니라 다양한 분야에서 사회적 비용의 소요를 야기한다. 오랜 기간 동안 뉴스 미디어와 온라인 커뮤니티가 사회에 미치는 부정적인 영향이 보고 되었으나, 이들은 여전히 우리 사회가 해결해야 할 당면 과제로 남아있다. 본 연구는 다양성(diversity) 증진 관점에서 개인의 정치적 이념과 일치하는 정보와 반대되는 정보를 함께 소비할 수 있도록 하는 AI 기반의 웹 인터페이스를 제안한다. 다양한 관점에서 정보를 소비하는 것의 중요성을 인식하는지에 대한 정도를 측정하는 5점 리커트 척도를 사용하여 사전, 사후 설문 조사를 시행한 결과, 본 인터페이스는 통계적으로 유의한 수준에서 참여자들로 하여금 다양한 관점에서의 정보 소비의 중요성 인지에 대한 정도를 높였다.                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |Political polarization promotes not only social divisions but also social costs in various fields. Although the negative impacts of news media and online communities on society have been reported for a long time, they remain challenges that society needs to address. This study proposes an AI-based web interface that allows users to consume both information consistent with their political ideology and information contrary to theirs from the perspective of promoting diversity. Pre and post-surveys scored on a 5-point Likert scale were used to measure the degree of awareness of the importance of consuming information from diverse perspectives. The interface helped to increase the awareness of users at a statistically significant level.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |SER000004124|2383-630X|정보과학회논문지               |우수등재       |2023   |09    |2023-09|50    |9    |50(9)    |[공학, 컴퓨터학]  |공학     |컴퓨터학 |Y                |한국어 |CRT002907441|김재훈               |한양대학교             |[CRT002907443, CRT002499136]                            |[박소현, 한경식]                |[아주대학교 소프트웨어학과, 한양대학교]                           |[political polarization, echo chamber, web interface, information consumption, 정치적 양극화, 반향실 효과, 웹 인터페이스, 정보 소비]                                                                                                                        |771  |776  |771-776|N/A|N/A|0        |https://www.kci.go.kr/kciportal/ci/sereArticleSearch/ciSereArtiView.kci?sereArticleSearchBean.artiId=ART002996493|N       |[]        |[]              |[]              |[]           |[]            |[]                 |[]               |[]             |[]            |[]           |[]            |[]          |\n",
      "|ART002996497|협동 로봇 모션 결함 데이터셋 구축을 위한 비전 기반 위치 편차 모의 결함 주입 방법       |Vision-based Position Deviation Fault Injection Method for Building a Collaborative Robot Motion Fault Dataset                                            |스마트팩토리의 핵심 설비인 협동 로봇에는 기기의 고장을 진단하기 위해 내부, 외부 센서로부터 데이터를 실시간으로 수집하고 결함을 예측하는 데이터 기반 결함 진단 방법이 도입되고 있다. 데이터 기반 결함 진단 방법은 학습을 위한 많은 양의 데이터가 필요하며, 특히 결함 상태로 레이블링된 대량의 데이터가 필수적으로 요구된다. 그러나, 산업 현장에서 실제 결함 데이터를 대량으로 얻기 어렵다. 따라서 본 논문에서는 비전 센서를 기반으로 협동 로봇 결함 상태의 출력을 정상 상태의 출력을 비교 분석하고, 분석된 출력 신호간의 편차를 바탕으로 모의 결함 주입 방법을 제안한다. 실제 결함 상태에서 수집된 협동 로봇 데이터는 제안하는 모의 결함 주입 상태에서 수집된 데이터로 대체 가능하다. 결함 주입 데이터로 학습된 모델의 성능과 실제 결함 데이터로 학습된 모델의 성능 비교 결과, 정확도의 경우 평균 0.97, 0.98로 차이가 거의 없음을 확인하여 제안하는 결함 주입 방법의 효용성을 검증하였다.                                                                                                                                                                                                                                                                                  |The data-based fault detection method, which collects data from internal and external sensors in real-time and predicts fault, is being applied to collaborative robots, which are key facilities in smart factories. The data-based fault detection method requires a large amount of data for learning, and in particular, a large amount of data labeled as a fault state is essential. However, it is difficult to obtain large amounts of actual fault data in industrial settings. Therefore, in this study, the output of the collaborative robot fault state based on a vision sensor was analyzed and compared with the output of the normal state, and a fault injection method was proposed based on the deviation between the analyzed output signals. Collaborative robot data collected in the actual fault state could be replaced with data collected in the proposed fault injection state. The comparison of the performance of the model trained with fault injection data and trained with actual fault data confirmed that there was almost no difference, with an average of 0.97 and 0.98 accuracy, thus verifying the effectiveness of the proposed fault injection method.                                                                                                                                                                                                                        |SER000004124|2383-630X|정보과학회논문지               |우수등재       |2023   |09    |2023-09|50    |9    |50(9)    |[공학, 컴퓨터학]  |공학     |컴퓨터학 |Y                |한국어 |CRT002977793|윤동희               |아주대학교             |[CRT002209834, CRT000843453]                            |[유동연, 이정원]                |[아주대학교, 아주대학교]                                          |[collaborative robot, building dataset, machine learning, fault detection, fault injection, 협동 로봇, 기계 학습, 데이터셋 구축, 결함 감지, 결함 주입]                                                                                                      |795  |804  |795-804|N/A|N/A|0        |https://www.kci.go.kr/kciportal/ci/sereArticleSearch/ciSereArtiView.kci?sereArticleSearchBean.artiId=ART002996497|N       |[]        |[]              |[]              |[]           |[]            |[]                 |[]               |[]             |[]            |[]           |[]            |[]          |\n",
      "|ART002996494|기계학습을 활용한 화합물의 약인성 간 손상 예측 방법 연구                               |Machine Learning-Based Approach for Predicting Drug-Induced Liver Injury of Chemical Compounds                                                            |약인성 간 손상은 임상시험용 의약품이 시장에 유통되는 것을 막는 요인 중 하나이다. 따라서 사전에 화합물의 약인성 간 손상 위험 평가가 필요하다. 안전성을 평가하기 위해 생체 내 (in vivo) 및 시험관 내 시험 방법(in vitro)이 사용되지만 이들은 시간과 비용이 많이 든다. 본 연구에서는 위의 문제를 극복하고자 random forest, light gradient boosting machine, logistic regression 모델을 제안한다. 모델은 입력으로 화합물의 분자 구조와 물리화학적 특징을 사용하고 출력으로 약인성 간 손상을 예측한다. 최적의 모델은 평가 지표에서 전반적으로 좋은 성능을 보인 random forest였다. 본 연구에서 제안된 모델은 신약 후보물질의 잠재적인 간 손상을 미리 파악함으로써 신약 개발 과정에 도움을 줄 수 있을 것으로 기대된다.                                                                                                                                                                                                                                                                                                                                                                                                                                                           |Drug-induced liver injury (DILI) is one of the factors constraining the distribution of investigational products on the market. Therefore, DILI risk of compounds should be assessed in advance. Although in vivo and in vitro methods can be used to test drug safety, both methods are labor-intensive, time consuming and expensive. In this study, we suggested random forest, light gradient boosting machine, logistic regression models to overcome the above problems. These models used molecular structure and physicochemical features as input to predict the DILI as output. The optimal model was random forest, which performed well for evaluation metrics overall. The proposed model is expected to help drug development process by identifying potential DILI of drug candidates in advance.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |SER000004124|2383-630X|정보과학회논문지               |우수등재       |2023   |09    |2023-09|50    |9    |50(9)    |[공학, 컴퓨터학]  |공학     |컴퓨터학 |Y                |한국어 |CRT002655876|이소연               |전남대학교             |[CRT002577276]                                          |[유선용]                        |[전남대학교]                                                      |[약인성 간 손상, 간 독성, 독성 예측, 기계학습, drug-induced liver injury, hepatotoxicity, toxicity prediction, machine learning]                                                                                                                            |777  |783  |777-783|N/A|N/A|0        |https://www.kci.go.kr/kciportal/ci/sereArticleSearch/ciSereArtiView.kci?sereArticleSearchBean.artiId=ART002996494|N       |[]        |[]              |[]              |[]           |[]            |[]                 |[]               |[]             |[]            |[]           |[]            |[]          |\n",
      "|ART002996501|옵저버 패턴에서 Kotlin 흐름 API의 도출                                                 |Derivation of Kotlin Flow API from Observer Pattern                                                                                                       |Kotlin 흐름은 비동기식 리액티브 스트림 데이터 처리를 위해 Kotlin 코루틴의 중단 함수를 기반으로 개발된 라이브러리이다. 그동안 널리 사용되어 왔던 무거운 라이브러리인 RxJava와는 달리 Kotlin 흐름은 코루틴의 중단 함수를 통해 자연스런 역압력 기능을 제공하기 때문에 최근 들어 리액티브 응용프로그램 개발 시 그 활용도가 높아지고 있다. 그럼에도 불구하고, 프로그래머는 흐름 API의 동작을 이해하는 데 어려움을 겪고 있는데, 이는 리액티브한 흐름 API의 동작이 일반적 API에 비해 익숙하지 않기 때문일 수도 있지만 흐름 API가 옵저버 패턴을 기본으로 하고 있다는 사실을 인지하지 못하고 있는 점 또한 주요 원인이라고 할 수 있다. 본 논문에서는 옵저버 패턴으로부터 흐름 API가 도출되는 과정을 단계별로 제시함으로써 Kotlin 흐름 API가 옵저버 패턴과 Kotlin 코루틴에 기초하여 정의되었으며, 이를 기반으로 리액티브 스트림 기능을 제공하고 있음을 보여준다.                                                                                                                                                                                                                                                                                                                     |Kotlin Flow is an asynchronous reactive stream library developed on top of Kotlin coroutine’s suspending functions. Recently, many reactive applications have increasingly adopted the Kotlin Flow instead of the well-known, but nevertheless heavy-weight, RxJava because it provides back-pressure support naturally by using coroutine’s suspending functions. However, the workings of the Flow API are difficult for most programmers to understand. One of the main reasons is due to the ignorance of the fact that its API is originated from the Observer Pattern, as well as its inherent reactive nature. In this paper, we show that Kotlin Flow API is nothing but a reactive version of the adapted Observer Pattern, together with Kotlin coroutines, by step-by-step illumination of how its API is derived from both the Observer Pattern and Kotlin’s coroutine.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |SER000004124|2383-630X|정보과학회논문지               |우수등재       |2023   |09    |2023-09|50    |9    |50(9)    |[공학, 컴퓨터학]  |공학     |컴퓨터학 |Y                |한국어 |CRT000305412|김정선               |한양대학교             |[]                                                      |[]                              |[]                                                                |[코틀린 흐름, 코루틴, 설계 패턴, 리액티브 스트림, Kotlin flow, Kotlin coroutine, design pattern, reactive stream]                                                                                                                                           |821  |826  |821-826|N/A|N/A|0        |https://www.kci.go.kr/kciportal/ci/sereArticleSearch/ciSereArtiView.kci?sereArticleSearchBean.artiId=ART002996501|N       |[]        |[]              |[]              |[]           |[]            |[]                 |[]               |[]             |[]            |[]           |[]            |[]          |\n",
      "|ART002996536|KoGPT2를 활용한 P-tuning의 효과적 성능 향상 기법 연구                                  |An Effective Method of Improving P-tuning Performance Using KoGPT2                                                                                        |최근 딥러닝을 이용한 자연어처리 분야에서 다양한 모델이 소개되는 가운데 BERT와 GPT 등 트랜스포머(Transformer) 기반의 사전훈련 언어모델(Pre-trained model)이 기본이 되고 있다. 트랜스포머 기반 모델의 파인-튜닝(Fine-tuning)은 전체 모델의 파라미터가 업데이트되어 우수한 성능을 보여주고 있다. 최근에는 적은 양의 파라미터를 업데이트하여 성능을 개선하는 P-tuning 방식이 등장하였다. 본 논문에서는 모델의 파라미터의 학습을 동결하여 적은 양의 파라미터만 업데이트하더라도 기존의 파인-튜닝과 비슷한 성능을 달성할 수 있는 피-튜닝 방식에서 프롬프트 인코더(Prompt-encoder)를 변경한 방법을 제안하였다. 성능 검증을 위하여 GPT-2 모델은 KoGPT2를 사용하였다. NSMC와 KorNLI 데이터셋을 이용한 분류 결과, 제안한 방법은 기존의 피-튜닝 방식과 비교하여 NSMC와 KorNLI 데이터셋으로 각각 4.56%와 11%의 정확도가 향상된 성능 향상 결과를 보였다.                                                                                                                                                                                                                                                                                                                               |Recently, various models of natural language processing using deep learning have been introduced, and transformer-based pre-trained models, such as BERT and GPT, have become the basic models. Fine-tuning transformer-based deep learning models can achieve excellent performance by updating the parameters of the entire model. Meanwhile, the P-tuning method, which can improve performance by updating a small number of parameters, has been introduced. In this study, we propose a method of changing the prompt-encoder from the P-tuning method, which could achieve performance similar to the existing fine-tuning method, even if only a small number of parameters were updated by freezing the learning of the model parameters. KoGPT2 was used as the GPT-2 model for performance verification. As a result of classifying using NSMC and KorNLI datasets, the proposed method showed enhanced performance using NSMC and KorNLI datasets, with an improved accuracy of 4.56% and 11%, respectively, compared to the existing P-tuning method.                                                                                                                                                                                                                                                                                                                                                         |A00398      |2383-6318|정보과학회 컴퓨팅의 실제 논문지|등재           |2023   |09    |2023-09|29    |9    |29(9)    |[공학, 컴퓨터학]  |공학     |컴퓨터학 |Y                |한국어 |CRT002664348|성열우               |남서울대학교           |[CRT002977824, CRT002977825, CRT000828382]              |[수라폰논상, 안기택, 김정길]    |[남서울대학교, 전북대학교, 남서울대학교]                          |[GPT-2, P-tuning, BERT, transformer, natural language processing, AI, GPT-2, P-tuning, BERT, Transformer, 자연어처리, 인공지능]                                                                                                                             |427  |431  |427-431|N/A|N/A|0        |https://www.kci.go.kr/kciportal/ci/sereArticleSearch/ciSereArtiView.kci?sereArticleSearchBean.artiId=ART002996536|N       |[]        |[]              |[]              |[]           |[]            |[]                 |[]               |[]             |[]            |[]           |[]            |[]          |\n",
      "|ART002996537|STP-MSPBEL 근사 알고리즘을 통한 무선 센서 네트워크에서의 효율적인 릴레이 노드 배치 방법|Efficient Relay Node Placement in Wireless Sensor Networks via STP-MSPBEL Approximation Algorithms                                                        |무선 센서 네트워크(wireless sensor networks)에서 많은 노드가 동시에 손상되어 네트워크가 여러 부분으로 분할되는 경우, 네트워크의 연결 단절이라는 큰 결함으로 이어질 수 있다. 이러한 상황에서, 네트워크의 연결을 비용과 시간 측면에서 효율적으로 복구하는 것이 매우 중요하다. 본 논문은 가장 적은 수의 릴레이 노드를 배치함으로써 연결을 완전히 복원하는 것을 목표로, 제한된 간선 길이 하의 스타이너 트리 문제(STP-MSPBEL)에 대한 근사 알고리즘들을 제시한다. STP-MSPBEL 문제는 트리의 모든 간선의 길이가 주어진 양수 R 이하임을 만족하면서, 스타이너 점의 개수를 최소화하는 문제이다. 이 문제는 NP-Complete임이 증명되었으며, 선행연구에 의해 다항 시간 알고리즘의 근사율이 5에서 3으로 줄어든 바 있다. 본 논문에서는 최악의 경우 근사율이 3임이 보장되면서, 선행연구보다 개선된 휴리스틱 알고리즘들을 제시하고, 그 성능을 비교실험을 통해 확인한다.                                                                                                                                                                                                                                                                                                                       |In wireless sensor networks, when many nodes are simultaneously damaged and the network is divided into several parts, it can lead to a significant failure known as network disconnection. In such situations, it is extremely important to efficiently recover the network connection in terms of cost and time. This paper presents approximate algorithms for the Steiner Tree Problem with the Minimum number of Steiner Points and Bounded Edge-Length(STP-MSPBEL) to fully restore the connection by placing the minimum number of relay nodes. The STP-MSPBEL problem aims to minimize the number of Steiner points while satisfying the condition that the length of all edges in the tree is no more than a given positive constant. This problem has been proven to be NP-Complete, and prior studies have shown a reduction in the approximation ratio of the polynomial time algorithm from 5 to 3. In this paper, we propose improved heuristic algorithms that guarantee an approximation ratio of 3 in the worst case and demonstrate their performance through comparison experiments with previous works.                                                                                                                                                                                                                                                                                                |A00398      |2383-6318|정보과학회 컴퓨팅의 실제 논문지|등재           |2023   |09    |2023-09|29    |9    |29(9)    |[공학, 컴퓨터학]  |공학     |컴퓨터학 |Y                |한국어 |CRT002977826|우은규               |대구경북과학기술원     |[CRT002977827, CRT002541447]                            |[채척, 신동훈]                  |[대구경북과학기술원, 대구경북과학기술원]                          |[STP-MSPBEL, wireless sensor networks, relay node placement, Steiner tree problem, Delaunay Triangulation, polynomial-time approximation scheme, STP-MSPBEL, 무선 센서 네트워크, 릴레이 노드 배치, 스타이너 트리 문제, 들로네 삼각분할, 다항 시간 근사 해법]|432  |437  |432-437|N/A|N/A|0        |https://www.kci.go.kr/kciportal/ci/sereArticleSearch/ciSereArtiView.kci?sereArticleSearchBean.artiId=ART002996537|N       |[]        |[]              |[]              |[]           |[]            |[]                 |[]               |[]             |[]            |[]           |[]            |[]          |\n",
      "|ART002996539|LSM-Tree의 읽기 성능 개선을 위한 병렬적 필터/인덱스 접근 기법                          |Parallel Filter/Index Access for Improving Read Performance of LSM-Tree                                                                                   |LSM-tree에서 키를 검색하는 과정은 하위 레벨부터 특정 키를 찾을 때까지 순차적으로 각 레벨을 탐색한다. 특정 레벨에서 키를 탐색할 때, 키의 존재여부를 미리 판단해주는 필터 블록을 우선적으로 접근한다. 만약, 탐색하고자 하는 레벨의 필터 블록이 메모리 상에 존재하지 않는 경우 저장 장치 I/O가 발생한다. 이 때, 연속된 레벨들의 필터 블록이 모두 메모리 상에 존재하지 않는 경우, 이전 레벨의 필터 테스트 완료를 대기한 후 다음 레벨의 필터 블록의 I/O를 발생시킨다. 또한, 필터 테스트 결과 키가 존재할 경우, 인덱스 블록을 순차적으로 접근한다. 이는 고성능 SSD의 높은 처리량을 활용하지 못하는 단점이 있다. 본 논문에서는 고성능 SSD를 활용하여 다중 레벨의 필터 블록 I/O를 병렬적으로 발생시켜 LSM-tree의 읽기 성능을 향상시키는 기법을 소개한다. 또한, 말단 레벨의 인덱스 블록까지 병렬적으로 읽어 LSM-tree의 읽기 성능을 향상시키고자 한다.                                                                                                                                                                                                                                                                                                                              |In a log-structured merge tree, the process of searching for a key involves sequentially searching each level from the bottom level until the specific key is found. When searching for a key at a particular level, the filter block, which determines whether the key exists, is accessed first. If the filter block for the level being searched is not present in the memory, a storage I/O occurs. If filter blocks for all consecutive levels are not present in the memory, the I/O for the filter block of the next level is generated after waiting for the filter test of the previous level to complete. Furthermore, if filter test results indicate existence of a key, the index block is accessed sequentially, which is a disadvantage of not utilizing the high throughput of high-performance SSDs. This paper introduces a technique for improving read performance of the log structure merge tree by generating multi-level filter block I/O in parallel using high-performance SSDs. Additionally, parallel reading of the last level index block is performed to further improve the read performance of the log structure merge tree.                                                                                                                                                                                                                                                              |A00398      |2383-6318|정보과학회 컴퓨팅의 실제 논문지|등재           |2023   |09    |2023-09|29    |9    |29(9)    |[공학, 컴퓨터학]  |공학     |컴퓨터학 |Y                |한국어 |CRT002977828|한정민               |성균관대학교           |[CRT002977829, CRT002387010]                            |[안민우, 정진규]                |[성균관대학교, 성균관대학교]                                      |[log-structured merge tree, high-performance SSD, sequential read, key-value store, LSM-tree, 고성능 SSD, 순차적인 읽기, 키-밸류 저장소]                                                                                                                    |444  |449  |444-449|N/A|N/A|0        |https://www.kci.go.kr/kciportal/ci/sereArticleSearch/ciSereArtiView.kci?sereArticleSearchBean.artiId=ART002996539|N       |[]        |[]              |[]              |[]           |[]            |[]                 |[]               |[]             |[]            |[]           |[]            |[]          |\n",
      "|ART003003721|Hybrid SARIMAX-LSTM 알고리즘을 이용한 시계열 자료 예측                                 |Predicting time series data using Hybrid SARIMAX-LSTM algorithm                                                                                           |최근에 시계열 자료 예측에 적합한 인공신경망인 LSTM (Long short-term memory) 알고리즘이 고안되면서, 계절형 자기회귀누적이동평균 (Seasonal auto regressive integrated moving average; SARIMA) 모형과 LSTM 알고리즘의 시계열 자료 예측 성능이 자주 비교되었다. 본 논문에서는 외생변수를 고려한 SARIMAX 모형을 LSTM 알고리즘과 결합한 Hybrid SARIMAX-LSTM 알고리즘을 제안하고자 한다. 모의실험과 실증분석을 통해 Hybrid SARIMAX-LSTM 알고리즘이 기존의 LSTM 알고리즘보다 예측 성능을 높이고, 분석 시간을 단축하는 데 효과적임을 입증하고자 한다.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |The seasonal autoregressive integrated moving average (SARIMA) model has long been used as a method for predicting time series data. As LSTM algorithm, which is an artificial neural network suitable for time series prediction, was devised, the prediction performance of SARIMA model and LSTM algorithm was often compared. In this paper, we propose a Hybrid SARIMAX-LSTM algorithm that combines SARIMAX, a traditional time series model that considers seasonality and exogenous variables, with LSTM algorithm. Through the results of simulation and empirical studies, we intend to prove that Hybrid SARIMAX-LSTM algorithm is effective in improving the prediction performance and shortening the analysis time compared to the existing LSTM algorithm.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |000605      |1598-9402|한국데이터정보과학회지         |우수등재       |2023   |09    |2023-09|34    |5    |34(5)    |[자연과학, 통계학]|자연과학 |통계학   |Y                |한국어 |CRT002725243|홍난영               |한국외국어대학교       |[CRT002987505, CRT000883067]                            |[이윤재, 이태욱]                |[한국외국어대학교, 한국외국어대학교]                              |[계절성, 시계열, 외생변수, LSTM, SARIMA, Exogenous variable, LSTM, SARIMA, seasonality, time series]                                                                                                                                                        |697  |709  |697-709|N/A|N/A|0        |https://www.kci.go.kr/kciportal/ci/sereArticleSearch/ciSereArtiView.kci?sereArticleSearchBean.artiId=ART003003721|N       |[]        |[]              |[]              |[]           |[]            |[]                 |[]               |[]             |[]            |[]           |[]            |[]          |\n",
      "+------------+---------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+---------+-------------------------------+---------------+-------+------+-------+------+-----+---------+------------------+---------+---------+-----------------+-------+------------+---------------------+-----------------------+--------------------------------------------------------+--------------------------------+------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----+-----+-------+---+---+---------+-----------------------------------------------------------------------------------------------------------------+--------+----------+----------------+----------------+-------------+--------------+-------------------+-----------------+---------------+--------------+-------------+--------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "db_name = \"kci_api\"\n",
    "collection_name = \"kci_202310\"\n",
    "\n",
    "mongo_data = list(client[db_name][collection_name].find({}))\n",
    "\n",
    "# object_id 제거\n",
    "schema = StructType([StructField(field, StringType(), True) for field in mongo_data[0].keys() if field != \"_id\"])\n",
    "spark_df = spark.createDataFrame(mongo_data, schema=schema)\n",
    "\n",
    "spark_df.show(truncate=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0f95c55-e752-49a9-ab14-6939bfce2aeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- articleID: string (nullable = true)\n",
      " |-- titleKor: string (nullable = true)\n",
      " |-- titleEng: string (nullable = true)\n",
      " |-- abstractKor: string (nullable = true)\n",
      " |-- abstractEng: string (nullable = true)\n",
      " |-- journalID: string (nullable = true)\n",
      " |-- issn: string (nullable = true)\n",
      " |-- journalName: string (nullable = true)\n",
      " |-- kciRegistration: string (nullable = true)\n",
      " |-- pubYear: string (nullable = true)\n",
      " |-- pubMon: string (nullable = true)\n",
      " |-- pubDate: string (nullable = true)\n",
      " |-- volume: string (nullable = true)\n",
      " |-- issue: string (nullable = true)\n",
      " |-- vol_issue: string (nullable = true)\n",
      " |-- category: string (nullable = true)\n",
      " |-- category1: string (nullable = true)\n",
      " |-- category2: string (nullable = true)\n",
      " |-- articleRegularity: string (nullable = true)\n",
      " |-- laguage: string (nullable = true)\n",
      " |-- author1ID: string (nullable = true)\n",
      " |-- author1Name: string (nullable = true)\n",
      " |-- author1Inst: string (nullable = true)\n",
      " |-- author2IDs: string (nullable = true)\n",
      " |-- author2Names: string (nullable = true)\n",
      " |-- author2Insts: string (nullable = true)\n",
      " |-- keywords: string (nullable = true)\n",
      " |-- fpage: string (nullable = true)\n",
      " |-- lpage: string (nullable = true)\n",
      " |-- page: string (nullable = true)\n",
      " |-- doi: string (nullable = true)\n",
      " |-- uci: string (nullable = true)\n",
      " |-- citations: string (nullable = true)\n",
      " |-- url: string (nullable = true)\n",
      " |-- verified: string (nullable = true)\n",
      " |-- refereceID: string (nullable = true)\n",
      " |-- refereceTypeCode: string (nullable = true)\n",
      " |-- refereceTypeName: string (nullable = true)\n",
      " |-- refereceTitle: string (nullable = true)\n",
      " |-- refereceAuthor: string (nullable = true)\n",
      " |-- refereceConfJournal: string (nullable = true)\n",
      " |-- referecePublisher: string (nullable = true)\n",
      " |-- referecePubDate: string (nullable = true)\n",
      " |-- refereceVolume: string (nullable = true)\n",
      " |-- refereceIssue: string (nullable = true)\n",
      " |-- refereceVolIss: string (nullable = true)\n",
      " |-- referecePage: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bbe64fc-5b84-44e1-b198-9494b01a3e73",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
