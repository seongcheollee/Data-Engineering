{"cells":[{"cell_type":"markdown","source":["# 구조적 API 개요\n\n- <strong>데이터 흐름을 정의</strong>하는 기본 추상화 개념\n- <strong>비정형 로그 파일</strong>부터 <strong>반정형 CSV파일</strong>, <strong>매우 정형적인 파케이파일</strong>까지 다양한 유형의 데이터를 처리할 수 있다.\n- 구조적 API에는 다음과 같은 세 가지 분산 컬렉션 API가 있음\n  - DataSet\n  - DataFrame\n  - SQL 테이블과 뷰"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7f9714f6-b884-4b8f-ae4c-9b7962e7966a"}}},{"cell_type":"markdown","source":["## DataFrame과 Dataset\n- 잘 정의된 로우와 컬럼을 가지는 분산 테이블 형태의 컬렉션\n- 모든 로우는 같은 데이터 타입 정보를 갖고 있어야함\n- 결과를 생성하기 위해 어떤 데이터에 어떤 연산을 적용해야 하는지 정의하는 지연 연산의 실행 계획이며 불변성을 가짐"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6688068d-1a61-4c9a-9dce-1ecf7384ba14"}}},{"cell_type":"markdown","source":["## 스키마\n- 분산 컬렉션에 저장할 <strong>컬럼과 데이터 타입</strong>을 정의하는 방법\n- 스키마는 데이터 소스에서 얻거나(schema-on-read) 직접 정의할 수 있음\n- <strong>어떤 데이터 타입이 어느 위치에</strong> 있는지 정의하는 방법이 필요함"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3dfdbb87-b095-4b9f-9aea-e1e2bb0a37ac"}}},{"cell_type":"markdown","source":["## 스파크의 구조적 데이터 타입 개요\n- 스파크는 <strong>실행 계획 수립과 처리에 사용하는 자체 데이터 타입 정보를 갖고 있는 카탈리스트 엔진</strong>을 사용함\n  - 카탈리스트 엔진은 다양한 <strong>실행 최적화</strong> 기능 제공\n- 스파크는 자체 데이터 타입을 지원하는 <strong>여러 언어 API와 직접 매핑되며, 각 언어에 대한 매핑 테이블</strong>을 갖고 있음"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"642806d0-f204-4c61-80fe-762fbcff0e3d"}}},{"cell_type":"code","source":["df=spark.range(500).toDF('number')\ndf.select(df['number']+10)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d8e69f0c-257a-4a1a-ad13-8769bc46b05a"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[1]: DataFrame[(number + 10): bigint]</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[1]: DataFrame[(number + 10): bigint]</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["----\n- 위 예제는 파이썬이 아닌 스파크 덧셈 연산을 수행\n- 스파크가 지원하는 언어를 이용해 작성된 표현식을 <strong>카탈리스트 엔진에서 스파크의 데이터 타입으로 변환</strong>해 명령을 처리하기 때문"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f9d81494-7f7d-4626-a5a7-363e9626b139"}}},{"cell_type":"markdown","source":["## DataFrame과 Dataset 비교\n- 본질적으로 구조적 API에는 <strong>'비타입형'인 DataFrame과 '타입형'인 Dataset</strong>이 있음\n  - DataFrame에도 데이터 타입이 있음\n  - 하지만 스키마에 명시된 <strong>데이터 타입의 일치 여부를 런타임</strong>에 확인(DataFrame)하냐 / <strong>컴파일 타임</strong>에 확인(Dataset)하냐의 차이가 있음 \n- 스파크 DataFrame은 <strong>Row 타입으로 구성된 DataSet</strong>임\n  - Row 타입은 스파크가 사용하는 <strong>'연산에 최적화된 인메모리 포맷'</strong>의 내부적인 표현 방식임\n  - Row 타입을 사용하면 가비지 컬렉션과 객체 초기화 부하가 있는 <strong>JVM 데이터 타입을 사용하는 대신 자체 데이터 포맷을 사용하므로 매우 효율적인 연산</strong>이 가능\n  - <strong>어떤 언어 API를 사용하더라도 동일한 효과와 효율성</strong>을 얻을 수 있음\n- 만약 컴파일 타임에 엄격한 데이터 검증이 필요하다면 DataSet 사용"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fe0c834d-aaa8-4d46-afba-fc3951c25955"}}},{"cell_type":"markdown","source":["## 컬럼\n- 컬럼은 다음과 같은 값을 표현\n  - 단순 데이터 타입(정수형, 문자열 등)\n  - 복합 데이터 타입(배열, 맵 등)\n  - null\n- 스파크는 데이터 타입의 모든 정보를 추적하며 다양한 컬럼 변환 방법을 제공함"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"dca1da3b-b8a3-4f84-9b5c-3427f93aa9d5"}}},{"cell_type":"markdown","source":["## 로우\n- 로우는 데이터 레코드\n- SQL, RDD, 데이터소스에서 얻거나 직접 만들 수 있음"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5224ad47-c7d6-46ef-a79a-16da456754f6"}}},{"cell_type":"code","source":["#range 메서드를 사용해 Dataframe 생성 -> Row 객체로 이루어진 배열 반환\nspark.range(2).collect()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a7e1fe1c-b2ae-46b2-87ec-b57b134df405"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[3]: [Row(id=0), Row(id=1)]</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[3]: [Row(id=0), Row(id=1)]</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## 스파크 데이터 타입\n- 스파크는 여러 내부 데이터 타입을 가지고 있음"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"27778c65-0430-4796-86d9-947dffe094c7"}}},{"cell_type":"code","source":["from pyspark.sql.types import *\n\na= ByteType()\nb= StringType()\nc= BinaryType()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d9231216-25b7-4077-b3dc-129950b30e94"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## 구조적 API의 실행 과정\n1. DataFrame/ Dataset/ SQL을 이용해 코드를 작성한다.\n2. 정상적인 코드라면 스파크가 논리적 실행 계획으로 변환한다.\n3. 스파크는 논리적 실행 계획을 물리적 실행 계획으로 변환하며, 그 과정에서 추가적인 최적화를 할 수 있는지 확인한다.\n4. 스파크는 클러스터에서 물리적 실행 계획(RDD 처리)을 실행한다.\n\n<img src=\"https://user-images.githubusercontent.com/12586821/54877777-a72ebe80-4e66-11e9-993c-de5cd4d707d0.jpg\" width=80% height=80%/>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"363faff5-def9-49ea-a042-7e8cdcab1bf1"}}},{"cell_type":"markdown","source":["### 논리적 실행 계획\n1. 사용자 코드 -> 검증 전 논리적 실행 계획으로 변환\n  - 사용자의 다양한 표현식을 <strong>최적화된 버전</strong>으로 변환함\n  - <strong>코드의 유효성과 테이블이나 컬럼의 존재 여부만을 판단</strong>하는 과정\n  - 아직 실행 계획을 검증하지 않은 상태<br><br>\n2. 분석기\n  - <strong>컬럼과 테이블을 검증</strong>하기 위해 카탈로그, 모든 테이블의 저장소, DataFrame의 정보 활용\n  - 필요한 테이블이나 컬럼이 카탈로그에 없다면 검증 전 논리적 실행 계획이 만들어지지 않음<br><br>\n3. 분석기에서의 검증 결과는 <strong>카탈리스트 옵티마이저</strong>로 전달 \n  - 카탈리스트 옵티마이저는 <strong>논리적 실행 계획을 최적화</strong>하는 규칙의 모음<br><br>\n4. <strong>최적화된 논리적 실행 계획</strong> 완성"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a514d4a5-caec-48ed-95a3-48affc951060"}}},{"cell_type":"markdown","source":["### 물리적 실행 계획\n- 스파크 실행 계획이라고도 불림\n- 논리적 실행 계획을 클러스터 환경에서 실행하는 방법을 정의\n\n\n1. <strong>다양한 물리적 실행 전략<strong> 생성\n2. <strong>비용 모델</strong>을 이용하여 비교한 후 <strong>최적의 전략 선택<strong>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6187e1e9-1121-4267-aa4b-e920686e94a3"}}},{"cell_type":"markdown","source":["### 실행\n- 물리적 실행 계획을 선정한 다음 저수준 프로그래밍 인터페이스인 RDD를 대상으로 모든 코드 실행\n- 스파크는 런타임에 전체 task나 stage를 제거할 수 있는 자바 바이트 코드를 생성해 추가적인 최적화 수행"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"82186a8c-75eb-488c-aef5-a9bb000ec25d"}}}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Chapter4","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":555091871453708}},"nbformat":4,"nbformat_minor":0}
